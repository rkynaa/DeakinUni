{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">SIT 112 - Data Science Concepts</span>\n",
    "\n",
    "---\n",
    "Lecturer: Sergiy Shelyag | sergiy.shelyag@deakin.edu.au<br />\n",
    "\n",
    "School of Information Technology, <br />\n",
    "Deakin University, VIC 3215, Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">Practical Session 9: k-Nearest Neighbors</span> \n",
    "**The purpose of this session is to demonstrate:**\n",
    "\n",
    "k-Nearest Neighbors classification technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\">k-Nearest Neighbours Classification</span> \n",
    "\n",
    "kNN is a non-parametric classification technique which is extensively used in practice. Its input consists of the `k` closest training examples and the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "**Please note that kNN is different from K-means.** K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster be close to each other. It is unsupervised because the points have no external classification. kNN is a classification algorithm that in order to determine the classification of a point, combines the class of the k nearest points. It is supervised because you are trying to classify a point based on the known label of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">kNN in Python</span> \n",
    "\n",
    "To be able to illustrate how we perform kNN classification in Python, we need some data first. Therefore we synthesize some data from 3 classes. We assume the data in each class comes from a multivariate random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "n_per_class = 50\n",
    "colors = ['green', 'blue', 'magenta']\n",
    "\n",
    "mean1 = [-5, 10]\n",
    "cov1 = [[1.5, 0], [0, 1.5]]\n",
    "mean2 = [0, 7]\n",
    "cov2 = [[1.5, 0], [0, 3]]\n",
    "mean3 = [-6, 6]\n",
    "cov3 = [[2, 0], [0, 1.5]]\n",
    "\n",
    "means = [mean1, mean2, mean3]\n",
    "covs = [cov1, cov2, cov3]\n",
    "\n",
    "x11, x12 = np.random.multivariate_normal(means[0], covs[0], n_per_class).T\n",
    "x21, x22 = np.random.multivariate_normal(means[1], covs[1], n_per_class).T\n",
    "x31, x32 = np.random.multivariate_normal(means[2], covs[2], n_per_class).T\n",
    "\n",
    "scale = 75\n",
    "alpha = 0.6\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(7, 7), dpi=100)\n",
    "ax.scatter(x11, x12, alpha=alpha, color=colors[0], s=scale, label=colors[0])\n",
    "ax.scatter(x21, x22, alpha=alpha, color=colors[1], s=scale, label=colors[1])\n",
    "ax.scatter(x31, x32, alpha=alpha, color=colors[2], s=scale, label=colors[2])\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"synthesized data for 3 classes\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to instantiate a kNN classifier from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "weights='uniform'\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k,weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass one array as training features and on array as training labels to the `knn` object. Therefore we have to put all the attributes together (also class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.r_[x11, x21, x31]\n",
    "x2 = np.r_[x12, x22, x32]\n",
    "X_train = np.c_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.r_[0*np.ones(n_per_class), 1*np.ones(n_per_class), 2*np.ones(n_per_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how kNN classifies a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "colors = ['green', 'blue', 'magenta']\n",
    "classes = np.unique(Y_train)\n",
    "for y in classes:\n",
    "    y = int(y)\n",
    "    ids = np.where(Y_train == y)[0]\n",
    "    print(ids)\n",
    "    ax.scatter(X_train[ids, 0], X_train[ids, 1], c=colors[y], alpha=alpha, s=scale, label='class ' + str(y))\n",
    "ax.legend()\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#X_test = np.array(((-3.2,-7.2),(-1,9)))\n",
    "X_test = np.array(((-7,10),(-1,9)))\n",
    "Y_pred = knn.predict(X_test)\n",
    "ax.scatter(X_test.T[0], X_test.T[1], marker=\"x\", s=scale, lw=2, c='k')\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))\n",
    "for i in range(0,len(Y_pred)):\n",
    "    print(\"Point \",i,\" is predicted as class {}\".format(colors[Y_pred.astype(int)[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Decision Boundry</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN effectively partitions the feature space into different sets and assigns the same class label to points belonging to the same partition. This partitioning changes as we change k. We illustrate this below. As you see bigger values of k, partition the space more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.05\n",
    "\n",
    "# Create colour maps\n",
    "cmap_light = ListedColormap(['#AAFFAA', '#AAAAFF', '#FFAAAA'])\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])\n",
    "\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will investigate the effect of `'k'` on decision boundaries. Lets train a classifier with `k=1` which means we only use the label of the closest point to predict the label of a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Prediction\n",
    "\n",
    "Play with the `X_test` and `k` to see how the classifier behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "#X_test = [-4, 8]\n",
    "X_test = np.array(((-7,10),(-1,9)))\n",
    "Y_pred = knn.predict(X_test)\n",
    "ax.scatter(X_test.T[0], X_test.T[1], marker=\"x\", s=scale, lw=2, c='k')\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))\n",
    "for i in range(0,len(Y_pred)):\n",
    "    print(\"Point \",i,\" is predicted as class {}\".format(colors[Y_pred.astype(int)[i]]))\n",
    "\n",
    "#ax.scatter(X_test[0], X_test[1], alpha=0.95, color='r', s=3*scale)\n",
    "\n",
    "#ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of predicting the class label for one point, we use our model to predict the labels of multiple points.\n",
    "\n",
    "First we generate some test data from the first class. This way we know the true class labels. Then we can use the `kNN` classifier to predict labels for the test data and get the predicted class labels. A measure of  accuracy for the classifier can be defined by comparing the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "n_test = 100\n",
    "X1_test, X2_test = np.random.multivariate_normal(mean1, cov1, n_test).T\n",
    "Y_true = 0 * np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.c_[X1_test, X2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times the classifier predicts the labels correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred == Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(Y_pred == Y_true) + 0.0) / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Repeat the previous experiment with a classifier which has bee trained with a different `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use kNN to implement a classifier on handwritten digits dataset introduced in prac7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "Y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "k = 10\n",
    "knn = neighbors.KNeighborsClassifier(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Y_pred == Y_test) / len(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Y_pred != Y_test\n",
    "n_false = np.sum(mask == True)\n",
    "print(n_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=n_false, figsize=(n_false, 1))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_test[mask][i, :].reshape(8, 8), cmap=plt.cm.binary)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"{}\".format(Y_pred[mask][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
