{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">SIT 112 - Data Science Concepts</span>\n",
    "\n",
    "---\n",
    "Lecturer: Sergiy Shelyag | sergiy.shelyag@deakin.edu.au<br />\n",
    "\n",
    "\n",
    "School of Information Technology, <br />\n",
    "Deakin University, VIC 3215, Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">Practical Session 7: K-Means Clustering with Scikit-Learn</span>\n",
    "\n",
    "**The purpose of this session is to teach you about:**\n",
    "\n",
    "1. Scikit-Learn Package\n",
    "2. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">1. Scikit-Learn</span>\n",
    "\n",
    "\n",
    "\n",
    "[Scikit-Learn](http://github.com/scikit-learn/scikit-learn) is a Python package designed to give access to **well-known** machine learning algorithms within Python code, through a **clean, well-thought-out API**. It has been built by hundreds of contributors from around the world, and is used across industry and academia.\n",
    "\n",
    "Scikit-Learn is built upon Python's [NumPy (Numerical Python)](http://numpy.org) and [SciPy (Scientific Python)](http://scipy.org) libraries, which enable efficient in-core numerical and scientific computation within Python. As such, scikit-learn is not specifically designed for extremely large datasets, though there is [some work](https://github.com/ogrisel/parallel_ml_tutorial) in this area.\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.1 Representation of Data in Scikit-learn</span>\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by\n",
    "discussing how data can be represented in order to be understood by the computer.  Along\n",
    "with this, we'll build on our matplotlib examples from the previous section and show some\n",
    "examples of how to visualize data.\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays.\n",
    "\n",
    "Scikit-Learn package include several datasets that you can load and start playing with them. You can consult with the documentation for details of the provided datasets. For example the [iris dataset.](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">2 Introducing K-Means</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means is an algorithm for **unsupervised clustering**: that is, finding clusters in data based on the data attributes alone (not the labels).\n",
    "\n",
    "K Means is a relatively easy-to-understand algorithm.  It searches for cluster centers which are the mean of the points within them, such that every point is closest to the cluster center it is assigned to.\n",
    "\n",
    "Let's look at how KMeans operates on the simple clusters we looked at previously. To emphasize that this is unsupervised, we'll not plot the colors of the clusters at first as if we don't have that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "col = ['r','g','b','y']\n",
    "\n",
    "\n",
    "n_centers = 4\n",
    "X, y = make_blobs(n_samples=300, centers=n_centers,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c = y, s=10)\n",
    "ax.set_title(\"simulated data without cluster labels\")\n",
    "\n",
    "print(np.shape(X))\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, it is relatively easy to pick out the four clusters. If you were to perform an exhaustive search for the different segmentations of the data, however, the search space would be exponential in the number of points. Fortunately, there is a well-known *Expectation Maximization (EM)* procedure which scikit-learn implements, so that KMeans can be solved relatively quickly. But before that, lets visualize the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(x,y):\n",
    "    '''\n",
    "    Compute the Euclidean distance between two vectors x and y\n",
    "    '''\n",
    "    dist = (np.array(x) - np.array(y))*(np.array(x) - np.array(y))\n",
    "    return np.sqrt(dist.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = X.shape[0]\n",
    "print(n_rows)\n",
    "\n",
    "X2 = []\n",
    "for k in range(n_centers):\n",
    "    X2.append(X[y == k])\n",
    "\n",
    "X3 = np.array([])\n",
    "X3 = np.append(X3, X2)\n",
    "X3 = X3.reshape(300, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances = np.zeros((n_rows, n_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Euclidean distance matrix using the Euclidean_distance function()\n",
    "\n",
    "for i in range(n_rows):\n",
    "    print(i, end=' ')\n",
    "    for j in range(n_rows):\n",
    "        euclidean_distances[i, j] = Euclidean_distance(X3[i, :], X3[j, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise this distance matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "cax = ax.imshow(euclidean_distances)\n",
    "cbar = fig.colorbar(cax)\n",
    "\n",
    "#plt.imshow(euclidean_distances)\n",
    "#plt.colorbar(cax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "est = KMeans(n_clusters=4)\n",
    "est.fit(X)\n",
    "\n",
    "y_kmeans = est.predict(X)\n",
    "# y_kmeans = est.labels_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='rainbow');\n",
    "ax.set_title('Estimated Clusters')\n",
    "\n",
    "\n",
    "print(y_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm identifies the four clusters of points in a manner very similar to what we would do by eye!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">2.1 The K-Means Algorithm: Expectation Maximization</span>\n",
    "\n",
    "\n",
    "K-Means is an example of an algorithm which uses an *Expectation-Maximization* approach to arrive at the solution.\n",
    "*Expectation-Maximization* is a two-step approach which works as follows:\n",
    "\n",
    "0. Guess some cluster centers\n",
    "0. Repeat until converged\n",
    "  0. Assign points to the nearest cluster center\n",
    "  0. Set the cluster centers to the mean\n",
    "   \n",
    "Let's quickly visualize this process. First execute the cell below to import `plot_kmeans_interactive()` which is needed for interactive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prac7_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac7_utils.plot_kmeans_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm will (often) converge to the optimal cluster centers.\n",
    "\n",
    "---\n",
    "## <span style=\"color:#0b486b\">2.2 Review</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the data\n",
    "\n",
    "n_centers = 4\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=n_centers,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=4, init='k-means++', n_init=1, max_iter=100, tol=0.0001, verbose=True)\n",
    "est.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(15, 6))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=est.labels_, s=50, cmap='rainbow');\n",
    "ax[0].set_title('True Clusters')\n",
    "\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n",
    "ax[1].set_title('True Clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Note</span>\n",
    "\n",
    "The cluster labels are not necesarily same as the true labels. This is not important for us. We are only interested in finding the clusters. Remeber KMeans is an unsupervised algorithm. How would it know the cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">2.4 KMeans for Digits Clustering</span>\n",
    "\n",
    "\n",
    "For a closer-to-real-world example, let's use a KMean to cluser the digits data provided by Scikit-Learn.\n",
    "\n",
    "Go to http://scikit-learn.org/stable/datasets/index.html#datasets\n",
    "to choose one dataset (Section 5.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example we can look at the data by simply plotting it. Why?\n",
    "\n",
    "Look at some of the data to see the difference between people's handwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example we can look at the data by simply plotting it. Why?\n",
    "\n",
    "Look at some of the data to see the difference between people's handwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "fig, axes = plt.subplots(nrows=3, ncols=10, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X[i, :].reshape(8, 8), cmap=plt.cm.binary)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    \n",
    "print(np.shape(axes.ravel()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup your KMeans estimator and fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=10)\n",
    "clusters = est.fit_predict(digits.data)\n",
    "est.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the cluster centers and confirm that even though we did not have access to the true labels, the cluster centers are recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 1))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(est.cluster_centers_[i].reshape(8, 8), cmap=plt.cm.binary)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that cluster lables might permute. Investigate and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 1\n",
    "mask = (clusters == j)\n",
    "digits.target[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "print(clusters==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>Some this notebook's material is taken from [Jake Vanderplas](http://www.vanderplas.com) presentation in PyCon 2015.</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
